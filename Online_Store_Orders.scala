package spark_programs

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object Online_Store_Orders {
  def main(args: Array[String]): Unit = {
    val spark =SparkSession.builder().master("local[*]").appName("online_store_orders").getOrCreate()
    val input =spark.read.option("header","true").csv("D:\\spark\\inputCSV\\store_orders.csv")
    input.printSchema()
    input.show(10,false)

//    ğŸ’¡ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ«ğğ¯ğğ§ğ®ğ ğ ğğ§ğğ«ğšğ­ğğ ğŸğ«ğ¨ğ¦ ğšğ¥ğ¥ ğ¨ğ«ğğğ«ğ¬.
    val total_revenue = input.agg(sum("total_amount").alias("total revenue"))
    total_revenue.printSchema()
    total_revenue.show()

//    ğŸ’¡ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­.

    val avg_amount = input.agg(avg("total_amount").alias("avg amount"))
    avg_amount.printSchema()
    avg_amount.show()

//    ğŸ’¡ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ­ğ¨ğ­ğšğ¥ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«.

    val highest_order_Amount1 = input.orderBy(desc("total_amount")).select(first("customer_id"),first("total_amount"))
    //or will use first or limit
    val highest_order_Amount = input.orderBy(desc("total_amount")).select("customer_id","total_amount").limit(1)
    highest_order_Amount.printSchema()
    highest_order_Amount.show()

//    ğŸ’¡ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¨ğ«ğğğ«ğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«.
    val number_of_orders = input.groupBy("customer_id").agg(count("order_id").alias("number_of_orders")).sort(desc("number_of_orders"))
    number_of_orders.show()

  }

}
